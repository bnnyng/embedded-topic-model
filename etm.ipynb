{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Preliminaries"
      ],
      "metadata": {
        "id": "4OUMEjuu9hyJ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "cUk-Xy8x9a-4"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from collections import Counter\n",
        "\n",
        "import gensim\n",
        "from gensim.models import Word2Vec\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "from sklearn.preprocessing import normalize"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "id": "0wDBpSct9kg8"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# File paths\n",
        "VOCAB_COL = 'content-inclusion-only'\n",
        "DOCUMENT_COL = 'content-cleaned'\n",
        "EMBED_FILE = 'test-embedMatrix-cognitive.txt'\n",
        "COMMENTS_FILE = 'comments-inclusion-bigrams20.json'\n",
        "MODEL_SAVE_PATH = 'test-ETM-cognitive.pth'\n",
        "BEST_MODEL_PATH = 'test-ETM-cognitive.pth'\n",
        "\n",
        "# Model hyperparameters\n",
        "T_HIDDEN_SIZE = 800\n",
        "\n",
        "# Training settings\n",
        "NUM_TOPICS = 10\n",
        "BATCH_SIZE = 64"
      ],
      "metadata": {
        "id": "NRT4Jph6z62Q"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Helper functions"
      ],
      "metadata": {
        "id": "O3LIe2LOKqrm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def read_embedding_file(file_path):\n",
        "    \"\"\"\n",
        "    Create an embedding matrix for the vocabulary from a text file.\n",
        "\n",
        "    Args:\n",
        "        file_path (str) : path to file\n",
        "    Returns:\n",
        "        numpy.array : VxD matrix, where V is vocab size and D is\n",
        "            dimension of the embedding space.\n",
        "    \"\"\"\n",
        "    vocab = []\n",
        "    vecs = []\n",
        "    with open(file_path, 'r') as f:\n",
        "        for line in f:\n",
        "            temp = line.strip().split()\n",
        "            vocab.append(temp[0])\n",
        "            vecs.append(list(map(float, temp[1:])))\n",
        "    matrix = np.array(vecs)\n",
        "    key_to_index = {v : i for i, v in enumerate(vocab)}\n",
        "    return matrix, key_to_index\n",
        "\n",
        "def process_docs_for_training(df, vocab_col, embed_dict):\n",
        "    docs = df[vocab_col].tolist()\n",
        "    doc_tokens = [doc.split() for doc in docs]\n",
        "    vocabulary = embed_dict.keys()\n",
        "\n",
        "    bag_of_words_docs = []\n",
        "    for token in doc_tokens:\n",
        "        bag_of_words = [0] * len(vocabulary)\n",
        "        word_counts = Counter(token)\n",
        "        for word, count in word_counts.items():\n",
        "            if word in embed_dict:\n",
        "                bag_of_words[embed_dict[word]] = count\n",
        "        bag_of_words_docs.append(bag_of_words)\n",
        "\n",
        "    normalized_bag_of_words = normalize(bag_of_words_docs, norm='l1', axis=1)\n",
        "    return bag_of_words_docs, normalized_bag_of_words\n",
        "\n",
        "def process_docs_for_inference(df, vocab_col, embed_dict):\n",
        "    docs = df[vocab_col].tolist()\n",
        "    doc_tensors = []\n",
        "    for doc in docs:\n",
        "        tensor = torch.zeros(len(embed_dict.keys()))\n",
        "        for word in doc.split():\n",
        "            if word in embed_dict:\n",
        "                tensor[embed_dict[word]] += 1\n",
        "        doc_tensors.append(tensor)\n",
        "    return torch.stack(doc_tensors)\n",
        "\n",
        "def get_top_words(model, vocab, top_n=10):\n",
        "    \"\"\"\n",
        "    Get the n top words for each topic.\n",
        "    \"\"\"\n",
        "    with torch.no_grad():\n",
        "        beta = model.get_beta().numpy()\n",
        "    top_words = {}\n",
        "    for topic_id in range(beta.shape[0]):\n",
        "        top_word_indices = np.argsort(beta[topic_id])[-top_n:]  # Get indices of top_n words\n",
        "        top_words[topic_id] = [vocab[i] for i in top_word_indices]\n",
        "    return top_words\n",
        "\n",
        "def get_top_doc_indices(model, bow_tensor, normalized_bow_tensor, n=10):\n",
        "    with torch.no_grad():\n",
        "        thetas, _ = model.get_theta(normalized_bow_tensor)\n",
        "        thetas = thetas.numpy()\n",
        "    top_docs = {}\n",
        "    num_topics = thetas.shape[1]\n",
        "    for topic_id in range(num_topics):\n",
        "        topic_proportion = thetas[:, topic_id]\n",
        "        top_doc_indices = np.argsort(topic_proportion)[-n:]\n",
        "        top_docs[topic_id] = top_doc_indices\n",
        "    return top_docs\n",
        "\n",
        "def get_top_docs(df, top_docs_dict):\n",
        "    for topic_id, indices in top_docs_dict.items():\n",
        "        df_topic = df.iloc[indices]\n",
        "        count = 1\n",
        "        print(f'\\nTopic: {topic_id}')\n",
        "        for i, row in df_topic.iterrows():\n",
        "            print(f'{count}. {row[\"content-no-tex\"]}')\n",
        "            print(f'{count}. {row[VOCAB_COL]}')\n",
        "            count += 1"
      ],
      "metadata": {
        "id": "NhP2RXaQKsOp"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model definition"
      ],
      "metadata": {
        "id": "teoDLggd9911"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Try simple ETM\n",
        "class ETM(nn.Module):\n",
        "    # TO DO: encoder dropout? default 0.5\n",
        "    def __init__(self, num_topics, embedding_matrix, t_hidden_size=100):\n",
        "        \"\"\"\n",
        "        Neural topic model using variational autoencoder.\n",
        "\n",
        "        Args:\n",
        "            num_topics (int) : number of topics\n",
        "            embedding_matrix (numpy.array) : VxD matrix, where V is the vocabulary\n",
        "                size and D is the embedding dimension\n",
        "            t_hidden_size (int) : dimension of hidden space q(theta)\n",
        "        \"\"\"\n",
        "        super(ETM, self).__init__()\n",
        "\n",
        "        # Model parameters\n",
        "        self.num_topics = num_topics\n",
        "        self.vocab_size = embedding_matrix.shape[0]\n",
        "        self.rho_size = embedding_matrix.shape[1]\n",
        "        self.t_hidden_size = t_hidden_size\n",
        "        # emsize?\n",
        "        # training?\n",
        "\n",
        "        # Optimization hyperparameters\n",
        "        self.enc_drop = 0.5\n",
        "        self.t_dropout = nn.Dropout(self.enc_drop)\n",
        "        self.theta_act = nn.ReLU()\n",
        "\n",
        "        # Define word embedding\n",
        "        self.rho = nn.Embedding(self.vocab_size, self.rho_size)\n",
        "        self.rho.weight.data.copy_(torch.from_numpy(embedding_matrix).float())\n",
        "        self.rho.weight.requires_grad = False # freeze weights\n",
        "\n",
        "        # Define topic embedding\n",
        "        self.alphas = nn.Linear(self.rho_size, num_topics, bias=False)\n",
        "\n",
        "        # Variational distribution for untransformed topic proportion\n",
        "        self.q_theta = nn.Sequential(\n",
        "            nn.Linear(self.vocab_size, self.t_hidden_size),\n",
        "            self.theta_act,\n",
        "            nn.Linear(self.t_hidden_size, self.t_hidden_size),\n",
        "            self.theta_act\n",
        "        )\n",
        "        self.mu_q_theta = nn.Linear(self.t_hidden_size, num_topics, bias=True)\n",
        "        self.log_sigma_q_theta = nn.Linear(self.t_hidden_size, num_topics, bias=True)\n",
        "\n",
        "    def reparameterize(self, mu, log_sigma):\n",
        "        \"\"\"\n",
        "        Returns a sample from the reparametrized Gaussian distribution TO DO\n",
        "\n",
        "        Args:\n",
        "            mu : mean\n",
        "            log_sigma : variance\n",
        "        \"\"\"\n",
        "        std = torch.exp(0.5 * log_sigma)\n",
        "        eps = torch.randn_like(std)\n",
        "        return eps.mul_(std).add_(mu)\n",
        "\n",
        "    def encode(self, bag_of_words):\n",
        "        \"\"\"\n",
        "        Get parameters of variational distribution for \\theta.\n",
        "\n",
        "        Args:\n",
        "            bag_of_words (torch.Tensor) : tensor of shape [batch_size] x [vocab_size]\n",
        "        \"\"\"\n",
        "        q_theta = self.q_theta(bag_of_words)\n",
        "        if self.enc_drop > 0:\n",
        "            q_theta = self.t_dropout(q_theta)\n",
        "        mu_theta = self.mu_q_theta(q_theta)\n",
        "        log_sigma_theta = self.log_sigma_q_theta(q_theta)\n",
        "        kl_theta = -0.5 * torch.sum(1 + log_sigma_theta - mu_theta.pow(2) - log_sigma_theta.exp())\n",
        "        return mu_theta, log_sigma_theta, kl_theta\n",
        "\n",
        "    def get_beta(self):\n",
        "        \"\"\"\n",
        "        Get 'traditional' topic (i.e., distribution over words) induced by word\n",
        "        embeddings \\rho and topic embedding \\alpha_k.\n",
        "        \"\"\"\n",
        "        logit = self.alphas(self.rho.weight) # torch.mm(self.rho.weight, self.alphas)\n",
        "        return F.softmax(logit, dim=0).transpose(1, 0) # softmax over vocab dimension\n",
        "\n",
        "    def get_theta(self, normalized_bag_of_words):\n",
        "        \"\"\"\n",
        "        Get topic proportion for the normalized \"bag of words\" document.\n",
        "        \"\"\"\n",
        "        mu_theta, log_sigma_theta, kl_theta = self.encode(normalized_bag_of_words)\n",
        "        z = self.reparameterize(mu_theta, log_sigma_theta) # topic assignment\n",
        "        theta = F.softmax(z, dim=1)\n",
        "        return theta, kl_theta\n",
        "\n",
        "    def decode(self, theta, beta):\n",
        "        \"\"\"\n",
        "        Get log-probabilities of a topic given a document, assigning small\n",
        "        values to avoid computing log of 0.\n",
        "\n",
        "        Args:\n",
        "            theta (torch.Tensor)\n",
        "            beta (torch.Tensor)\n",
        "        \"\"\"\n",
        "        results = torch.mm(theta, beta)\n",
        "        almost_zeros = torch.full_like(results, 1e-6)\n",
        "        results_without_zeros = results.add(almost_zeros)\n",
        "        return torch.log(results_without_zeros)\n",
        "\n",
        "    def forward(self, bag_of_words, normalized_bag_of_words, theta=None):\n",
        "        # Get topic proportions\n",
        "        if theta is None:\n",
        "            theta, kl_theta = self.get_theta(normalized_bag_of_words)\n",
        "        else:\n",
        "            kl_theta = None\n",
        "\n",
        "        # Get topic as a distribution over words\n",
        "        beta = self.get_beta()\n",
        "\n",
        "        # Get prediction loss\n",
        "        preds = self.decode(theta, beta)\n",
        "        reconstruction_loss = -(preds * bag_of_words).sum(1)\n",
        "        reconstruction_loss = reconstruction_loss.mean() # aggregate\n",
        "        return reconstruction_loss, kl_theta"
      ],
      "metadata": {
        "id": "Uw__UblX9-6D"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Import data"
      ],
      "metadata": {
        "id": "wGbk83oN9lsN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "embed_matrix, embed_dict = read_embedding_file(EMBED_FILE)\n",
        "vocabulary = embed_dict.keys()\n",
        "embed_matrix.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TDtBBhMq9mmV",
        "outputId": "9e06fcee-ec4b-402e-8a28-aa2d8b651638"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(652, 300)"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_comments = pd.read_json(COMMENTS_FILE)\n",
        "df = df_comments\n",
        "df.columns"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oZV7yM4GMwQz",
        "outputId": "dbe6077b-d297-4b7e-bdb6-b1d4576510cc"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['author', 'author-href', 'time', 'comment-href', 'comment-id',\n",
              "       'content', 'child-ids', 'blog', 'post-id', 'id', 'in-reply-to',\n",
              "       'in-reply-to-href', 'project-id', 'content-no-tex', 'tex-count',\n",
              "       'content-cleaned', 'content-inclusion-only', 'content-bigrams'],\n",
              "      dtype='object')"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "bow, normalized_bow = process_docs_for_training(df, VOCAB_COL, embed_dict)"
      ],
      "metadata": {
        "id": "QYycsFPa2_2p"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bow_tensor = torch.tensor(bow, dtype=torch.float32)\n",
        "normalized_bow_tensor = torch.tensor(normalized_bow, dtype=torch.float32)\n",
        "\n",
        "print(bow_tensor.shape, normalized_bow_tensor.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jS5pyGQu3tG9",
        "outputId": "2d5c8221-8777-49e6-9d59-0b1803c988c7"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([15924, 652]) torch.Size([15924, 652])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train model"
      ],
      "metadata": {
        "id": "GFcm_vv03UPN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = ETM(num_topics=NUM_TOPICS, embedding_matrix=embed_matrix, t_hidden_size=T_HIDDEN_SIZE)\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.002)"
      ],
      "metadata": {
        "id": "KMy6Mqg95lLn"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_for_epochs(model, optimizer, num_epochs, bow_tensor, normalized_bow_tensor,\n",
        "              batch_size=BATCH_SIZE, model_save_path=MODEL_SAVE_PATH):\n",
        "    best_loss = float('inf')\n",
        "    for epoch in range(num_epochs):\n",
        "\n",
        "        model.train()\n",
        "        total_loss = 0\n",
        "        num_batches = len(bow_tensor) // batch_size\n",
        "        print(f'Training for epoch {epoch+1} with {num_batches} batches...')\n",
        "\n",
        "        for i in range(num_batches):\n",
        "            batch_bow = bow_tensor[i*batch_size:(i+1)*batch_size]\n",
        "            batch_normalized_bow = normalized_bow_tensor[i*batch_size:(i+1)*batch_size]\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Forward pass\n",
        "            reconstruction_loss, kl_theta = model(batch_bow, batch_normalized_bow)\n",
        "\n",
        "            # Backward pass\n",
        "            loss = reconstruction_loss + (kl_theta if kl_theta is not None else 0)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            total_loss += loss.item()\n",
        "\n",
        "        avg_loss = total_loss / num_batches\n",
        "        print(f'Epoch {epoch+1} complete.')\n",
        "        print(f'\\tAverage loss: {avg_loss}')\n",
        "\n",
        "        # Check if the current epoch loss is the best\n",
        "        if avg_loss < best_loss:\n",
        "            best_loss = avg_loss\n",
        "            torch.save(model.state_dict(), model_save_path)\n",
        "\n",
        "\n",
        "train_for_epochs(\n",
        "    model, optimizer, num_epochs=50,\n",
        "    bow_tensor=bow_tensor, normalized_bow_tensor=normalized_bow_tensor\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o43oalPi3Tcd",
        "outputId": "6e258703-ace4-4614-c666-946ef442f848"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training for epoch 1 with 248 batches...\n",
            "Epoch 1 complete.\n",
            "\tAverage loss: 109.40112630782589\n",
            "Training for epoch 2 with 248 batches...\n",
            "Epoch 2 complete.\n",
            "\tAverage loss: 105.86757279980567\n",
            "Training for epoch 3 with 248 batches...\n",
            "Epoch 3 complete.\n",
            "\tAverage loss: 104.9123694204515\n",
            "Training for epoch 4 with 248 batches...\n",
            "Epoch 4 complete.\n",
            "\tAverage loss: 104.31167884026804\n",
            "Training for epoch 5 with 248 batches...\n",
            "Epoch 5 complete.\n",
            "\tAverage loss: 103.89229049990254\n",
            "Training for epoch 6 with 248 batches...\n",
            "Epoch 6 complete.\n",
            "\tAverage loss: 103.60667559408373\n",
            "Training for epoch 7 with 248 batches...\n",
            "Epoch 7 complete.\n",
            "\tAverage loss: 103.48274450917398\n",
            "Training for epoch 8 with 248 batches...\n",
            "Epoch 8 complete.\n",
            "\tAverage loss: 103.3148743875565\n",
            "Training for epoch 9 with 248 batches...\n",
            "Epoch 9 complete.\n",
            "\tAverage loss: 103.22208962901946\n",
            "Training for epoch 10 with 248 batches...\n",
            "Epoch 10 complete.\n",
            "\tAverage loss: 103.12413995496688\n",
            "Training for epoch 11 with 248 batches...\n",
            "Epoch 11 complete.\n",
            "\tAverage loss: 103.12527645787885\n",
            "Training for epoch 12 with 248 batches...\n",
            "Epoch 12 complete.\n",
            "\tAverage loss: 103.03063021936724\n",
            "Training for epoch 13 with 248 batches...\n",
            "Epoch 13 complete.\n",
            "\tAverage loss: 102.98191879641625\n",
            "Training for epoch 14 with 248 batches...\n",
            "Epoch 14 complete.\n",
            "\tAverage loss: 102.9145468127343\n",
            "Training for epoch 15 with 248 batches...\n",
            "Epoch 15 complete.\n",
            "\tAverage loss: 102.92125809577203\n",
            "Training for epoch 16 with 248 batches...\n",
            "Epoch 16 complete.\n",
            "\tAverage loss: 102.87209327759281\n",
            "Training for epoch 17 with 248 batches...\n",
            "Epoch 17 complete.\n",
            "\tAverage loss: 102.86742016576952\n",
            "Training for epoch 18 with 248 batches...\n",
            "Epoch 18 complete.\n",
            "\tAverage loss: 102.80719670941752\n",
            "Training for epoch 19 with 248 batches...\n",
            "Epoch 19 complete.\n",
            "\tAverage loss: 102.79822386464765\n",
            "Training for epoch 20 with 248 batches...\n",
            "Epoch 20 complete.\n",
            "\tAverage loss: 102.78365464364329\n",
            "Training for epoch 21 with 248 batches...\n",
            "Epoch 21 complete.\n",
            "\tAverage loss: 102.73613963588592\n",
            "Training for epoch 22 with 248 batches...\n",
            "Epoch 22 complete.\n",
            "\tAverage loss: 102.69738100421044\n",
            "Training for epoch 23 with 248 batches...\n",
            "Epoch 23 complete.\n",
            "\tAverage loss: 102.71538219144267\n",
            "Training for epoch 24 with 248 batches...\n",
            "Epoch 24 complete.\n",
            "\tAverage loss: 102.71267420245755\n",
            "Training for epoch 25 with 248 batches...\n",
            "Epoch 25 complete.\n",
            "\tAverage loss: 102.68395342365388\n",
            "Training for epoch 26 with 248 batches...\n",
            "Epoch 26 complete.\n",
            "\tAverage loss: 102.67529089220109\n",
            "Training for epoch 27 with 248 batches...\n",
            "Epoch 27 complete.\n",
            "\tAverage loss: 102.66931340002245\n",
            "Training for epoch 28 with 248 batches...\n",
            "Epoch 28 complete.\n",
            "\tAverage loss: 102.61746800330377\n",
            "Training for epoch 29 with 248 batches...\n",
            "Epoch 29 complete.\n",
            "\tAverage loss: 102.57539115413543\n",
            "Training for epoch 30 with 248 batches...\n",
            "Epoch 30 complete.\n",
            "\tAverage loss: 102.63476482514412\n",
            "Training for epoch 31 with 248 batches...\n",
            "Epoch 31 complete.\n",
            "\tAverage loss: 102.60565400892689\n",
            "Training for epoch 32 with 248 batches...\n",
            "Epoch 32 complete.\n",
            "\tAverage loss: 102.60411076391897\n",
            "Training for epoch 33 with 248 batches...\n",
            "Epoch 33 complete.\n",
            "\tAverage loss: 102.57321726891303\n",
            "Training for epoch 34 with 248 batches...\n",
            "Epoch 34 complete.\n",
            "\tAverage loss: 102.58202735839352\n",
            "Training for epoch 35 with 248 batches...\n",
            "Epoch 35 complete.\n",
            "\tAverage loss: 102.55218085935039\n",
            "Training for epoch 36 with 248 batches...\n",
            "Epoch 36 complete.\n",
            "\tAverage loss: 102.5374645263918\n",
            "Training for epoch 37 with 248 batches...\n",
            "Epoch 37 complete.\n",
            "\tAverage loss: 102.55472349351452\n",
            "Training for epoch 38 with 248 batches...\n",
            "Epoch 38 complete.\n",
            "\tAverage loss: 102.55177931631765\n",
            "Training for epoch 39 with 248 batches...\n",
            "Epoch 39 complete.\n",
            "\tAverage loss: 102.53119934758833\n",
            "Training for epoch 40 with 248 batches...\n",
            "Epoch 40 complete.\n",
            "\tAverage loss: 102.4959914145931\n",
            "Training for epoch 41 with 248 batches...\n",
            "Epoch 41 complete.\n",
            "\tAverage loss: 102.52053593051049\n",
            "Training for epoch 42 with 248 batches...\n",
            "Epoch 42 complete.\n",
            "\tAverage loss: 102.4808815833061\n",
            "Training for epoch 43 with 248 batches...\n",
            "Epoch 43 complete.\n",
            "\tAverage loss: 102.53330896746728\n",
            "Training for epoch 44 with 248 batches...\n",
            "Epoch 44 complete.\n",
            "\tAverage loss: 102.48256686425978\n",
            "Training for epoch 45 with 248 batches...\n",
            "Epoch 45 complete.\n",
            "\tAverage loss: 102.47041354640838\n",
            "Training for epoch 46 with 248 batches...\n",
            "Epoch 46 complete.\n",
            "\tAverage loss: 102.47141335087437\n",
            "Training for epoch 47 with 248 batches...\n",
            "Epoch 47 complete.\n",
            "\tAverage loss: 102.4655205972733\n",
            "Training for epoch 48 with 248 batches...\n",
            "Epoch 48 complete.\n",
            "\tAverage loss: 102.4734289415421\n",
            "Training for epoch 49 with 248 batches...\n",
            "Epoch 49 complete.\n",
            "\tAverage loss: 102.49382835818875\n",
            "Training for epoch 50 with 248 batches...\n",
            "Epoch 50 complete.\n",
            "\tAverage loss: 102.46780512409825\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Make inferences"
      ],
      "metadata": {
        "id": "572R40bE_kGQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "best_model = ETM(num_topics=NUM_TOPICS, embedding_matrix=embed_matrix, t_hidden_size=T_HIDDEN_SIZE)\n",
        "best_model.load_state_dict(torch.load(BEST_MODEL_PATH))\n",
        "model.eval()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gTv-PYBm_vC9",
        "outputId": "7c669e64-9317-4b90-c3d1-ab89efb8034a"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ETM(\n",
              "  (t_dropout): Dropout(p=0.5, inplace=False)\n",
              "  (theta_act): ReLU()\n",
              "  (rho): Embedding(652, 300)\n",
              "  (alphas): Linear(in_features=300, out_features=10, bias=False)\n",
              "  (q_theta): Sequential(\n",
              "    (0): Linear(in_features=652, out_features=800, bias=True)\n",
              "    (1): ReLU()\n",
              "    (2): Linear(in_features=800, out_features=800, bias=True)\n",
              "    (3): ReLU()\n",
              "  )\n",
              "  (mu_q_theta): Linear(in_features=800, out_features=10, bias=True)\n",
              "  (log_sigma_q_theta): Linear(in_features=800, out_features=10, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "doc_tensors = process_docs_for_inference(df, VOCAB_COL, embed_dict)\n",
        "normalized_doc_tensors = F.normalize(doc_tensors, p=1, dim=1) # TO DO: check args\n",
        "print(doc_tensors.shape, normalized_doc_tensors.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hGhjPLokAvfT",
        "outputId": "cc0343ae-ed35-4f4c-feab-c54ca969243f"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([15924, 652]) torch.Size([15924, 652])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Get top words\n",
        "get_top_words(model, list(embed_dict.keys()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cz19feJXDSLz",
        "outputId": "3b2a7a91-f09c-46c1-f4e2-a420474fc846"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{0: ['need',\n",
              "  'enough',\n",
              "  'know',\n",
              "  'right',\n",
              "  'find',\n",
              "  'case',\n",
              "  'work',\n",
              "  'see',\n",
              "  'get',\n",
              "  'think'],\n",
              " 1: ['seems',\n",
              "  'could',\n",
              "  'find',\n",
              "  'should',\n",
              "  'think',\n",
              "  'case',\n",
              "  'see',\n",
              "  'get',\n",
              "  'will',\n",
              "  'would'],\n",
              " 2: ['case',\n",
              "  'could',\n",
              "  'work',\n",
              "  'think',\n",
              "  'using',\n",
              "  'should',\n",
              "  'use',\n",
              "  'will',\n",
              "  'would',\n",
              "  'also'],\n",
              " 3: ['know',\n",
              "  'example',\n",
              "  'use',\n",
              "  'using',\n",
              "  'think',\n",
              "  'work',\n",
              "  'case',\n",
              "  'see',\n",
              "  'get',\n",
              "  'also'],\n",
              " 4: ['need',\n",
              "  'seems',\n",
              "  'different',\n",
              "  'could',\n",
              "  'should',\n",
              "  'think',\n",
              "  'see',\n",
              "  'would',\n",
              "  'will',\n",
              "  'also'],\n",
              " 5: ['example',\n",
              "  'seems',\n",
              "  'case',\n",
              "  'should',\n",
              "  'think',\n",
              "  'could',\n",
              "  'get',\n",
              "  'would',\n",
              "  'will',\n",
              "  'also'],\n",
              " 6: ['most',\n",
              "  'using',\n",
              "  'example',\n",
              "  'case',\n",
              "  'could',\n",
              "  'use',\n",
              "  'should',\n",
              "  'will',\n",
              "  'also',\n",
              "  'would'],\n",
              " 7: ['case',\n",
              "  'use',\n",
              "  'actually',\n",
              "  'find',\n",
              "  'using',\n",
              "  'think',\n",
              "  'work',\n",
              "  'see',\n",
              "  'get',\n",
              "  'also'],\n",
              " 8: ['seems',\n",
              "  'need',\n",
              "  'using',\n",
              "  'work',\n",
              "  'could',\n",
              "  'different',\n",
              "  'use',\n",
              "  'get',\n",
              "  'think',\n",
              "  'should'],\n",
              " 9: ['made',\n",
              "  'case',\n",
              "  'done',\n",
              "  'need',\n",
              "  'could',\n",
              "  'should',\n",
              "  'get',\n",
              "  'think',\n",
              "  'will',\n",
              "  'would']}"
            ]
          },
          "metadata": {},
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Get top documents\n",
        "top_docs = get_top_doc_indices(model, bow_tensor, normalized_bow_tensor)\n",
        "get_top_docs(df, top_docs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fIlhyOkfA1cP",
        "outputId": "64da35e8-d356-4b1a-a836-c65e64d0d6f6"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Topic: 0\n",
            "1. Here’s another suggestion, just using the usual Riemann zeta function: our explicit formula for  is  . Now maybe we can work with a small set of values  , specifically chosen to nullify the effect of a large proportion of the zeros up to height, say,  or so. How? Well here is a first attempt: consider  (and of course we will want to show, say, that  is “large”, so that one of the intervals  has loads of primes). Applying the explicit formula, we find that  . Notice here that  is a fragment of the Riemann zeta function, and its closeness to  can be determined through the usual integration-by-parts analytic continuation  \\{t\\}tRe(s) > 0\\zeta(\\rho) = 01/(\\rho-1)\\rhoF(x) = x(1 + \\cdots + 1/n) - \\sum_\\rho x^\\rho/\\rho(\\rho-1) + EE\\rho^2$ in the denominator is good news, assuming I haven’t made an error. Assuming this is all correct, the skeptic in me says that when the dust has settled we get no gain whatsoever.\n",
            "1.     using       explicit        maybe   work         specifically                say          attempt consider      will want  show say                applying  explicit   find                                                     assuming   made   assuming    correct  skeptic   says        get   \n",
            "2. [Dear Ernie, sorry for not getting back to you earlier, I’ve been unusually distracted in the last two weeks.] I totally understand, especially given the ICM conference and all. [I am unsure about the wording, so please check.] The wording looks fine to me; and I’m sure David and David would appreciate seeing that they are mentioned (and perhaps when they go to apply for grad school… etc.). It’s looking like we might be able to prove a fast algorithm for  , as hoped; I'll know for sure in another two weeks or so (after I return from India, and get to meet with them again). … I see that you list the Borodin and Moenk paper, but didn't see that you replaced the use of matrices in the proof of Lemma 3.1 with a reference to it. Now that I think about it, I think I like including the matrix argument instead of just doing a citation. The fact is that everyone is familiar with Strassen's matrix algorithm, while few people know the argument in B-M; and so, the present argument in Lemma 3.1 is more self-contained (given the Strassen matrix alg.), and would be but one or two lines shorter anyways, if we just used the result in B-M (instead of matrices). … I would like to look over the paper again carefully before we submit it; but that will be a week or two from now, as I will be leaving for India tomorrow.\n",
            "2. dear  sorry   getting back              totally understand especially         unsure     please     fine     sure    would appreciate       perhaps     apply    etc  looking   might  able  prove          know  sure       after      get        see           see     use     proof            think    think      argument instead       fact     familiar        know  argument       argument             would              result   instead     would   look            will          will     \n",
            "3. Thanks for the link. Some more comments to the references: – [1]: Remove “Pages”. – [3]: Remove “pp.”. – [11]: Remove “pp.”. – [21] Remove “pp.”. [Changed, thanks -T.]\n",
            "3.                             \n",
            "4. Using the Hensley-Richards sieve for k=10,000,000 gives an admissible sequence of diameter 175,225,874 for m=3.\n",
            "4. using      gives        \n",
            "5. Just to clarify the condition I wrote down, suppose that  . In that case, let  and  be the characteristic functions of  and  . Then for  to belong to  we require that  is constant on  ,  ,  , and  . From the second condition, we must have that  is 1 everywhere on  , and also that it cannot exceed 1 on  or 0 on  . So the only thing left to choose is the value on  . If we take this to be 0 then we get  and if we take it to be 1 then we get  . So we recover Frankl’s conjecture, and in particular do not get an up-set.\n",
            "5.   clarify  condition    suppose      case let                 belong                      condition  must      everywhere     also   cannot                         take       get     take       get          particular   get  \n",
            "6. In page 65, the line before the last is too long. In page 66, there is a typo in the second term of the E-polynomial. [Corrected, thanks – T.]\n",
            "6.                             \n",
            "7. That’s an interesting example. It’s not obvious that one actually needs both  and  to be close to an even integer for the partial sums along multiples of  to get large, however. Isn’t it enough if we just have  and  with  and  very close to each other? Then the parities of  and  will be the same for a long time. But the pigeonhole argument gives that in time  rather than time  .\n",
            "7.   interesting example   obvious   actually needs                     get  however   enough                        will           argument gives     rather    \n",
            "8. I used the first formula for  after equation (2) [ 1st thread ] to evaluate numerically  . Among the (numerical) expressions in the first thread springing from (2) to (3), it’s the one with value closest to zero. Phi(X) = sum(Y=1, 50, (2*Pi*Pi*(Y^4)*exp(9*X) -3*Pi*(Y^2)*exp(5*X))*exp(-Pi*(Y^2)*exp(4*X))) intnumgaussinit(800); // 800-point Gauss-Legendre method realprecision = 154 significant digits For the PARI/gp numerical integration code: z=2*z1; t=0.0; thet=Pi/8 – 1/(4*z); K=0.0; for(J=0, 256 , K=K+intnumgauss(X=J/128, (J+1)/128, Phi(X+I*thet)*exp(I*z*(X+I*thet)) )); print( real((K+conj(K)))/2) output: 1.809 E-127\n",
            "8.        after                                           method                             \n",
            "9. Why we need to choose the simplex such? What to change if you take ?\n",
            "9.   need           take \n",
            "10. If you add in edges for all progressions, you have unbounded discrepancy by Roth, and if you add in the quasi-progressions ($latex \\lfloor k \\alpha \\rfloor, k=1,\\dots,L) you get unbounded discrepancy by Beck.\n",
            "10.                             get    \n",
            "\n",
            "Topic: 1\n",
            "1. A pure probabilistic approach to identifying a prime number is tuned to using particular tools to show that success is likely after not-too-much time. However, testing one number after another chosen at random does throw away information – it will only be useful information if it can be used to make the next choice better-than-average: could this be possible? Or alternatively, is there a slightly more expensive algorithm for checking primality which yIelds such useful information if a number is found to be composite?\n",
            "1.  pure  approach         using particular tools  show    likely after   however    after        information   will   useful information            could   possible                useful information     found   \n",
            "2. Say there are four points: an equilateral triangle, and then one point in the center of the triangle. No three points are collinear. It seems to me that the windmill can not use the center point more than once! As soon as it hits one of the corner points, it will cycle indefinitely through the corners and never return to the center point. I must be missing something here…\n",
            "2. say                        seems        use                  will  indefinitely     never       must  missing something \n",
            "3. 868. Density increment. 4. But there is a positive probability that  and  Moreover, the density of  inside the subspace  (I hope it’s easy to guess what that means) is at most  or we’re done. So, turning things round, there is in fact a positive probability that the conditional probability in 3 is somewhat less than  This is a bit easier to say in the new language. If  are disjoint sets, let’s write  for the sequence fragment that takes the value  on  (so  ). And if  then let’s write  for the extension of this fragment that takes value  at each  . For any given  let  be the intersection of all the sets  . That is,  consists of all  such that  for every  . It is easy to check (and I’ve done so — I promise) that  is a 12-set. In fact, I think I’ll need to give a quick proof of that later on. At this point we use the multidimensional Sperner type result to say that if we choose  at random then the expected density of  is positive (meaning bounded below by a positive constant that depends only on  and the sizes of the sets  This is saying that if  is a dense 12-set, then a positive density of all possible m-dimensional combinatorial spaces (with very small wildcard sets) is contained in  . This bit is not thoroughly checked, but I don’t think there’s much doubt that it can be with a bit of effort. Now let’s suppose that  Let  be the 1-set of  and let  be the 2-set of  . Then we must have that the union of  and any subcollection of the  belongs to  , and similarly for  and  . And the converse is true too, since  is a 12-set. Let  be the set of all  such that  (to use my notation above), and similarly for  . Then  is (naturally identified with) the set of all  with 1-set in  and 2-set in  So it really is a 12-set in  Now let’s fix a choice of  such that  has positive density. And let’s write  for  , and similarly for  and  . For every  the subspace of all points  contains at most  points in  or we have our density increase on a subspace. Therefore, if we choose a random  , the proportion of  such that  is on average at most  . So choose  such that this proportion is at most  . Now let’s suppose that  has near maximal density  in  , which I will denote by  . This set naturally partitions into four parts, according to whether the 1-set/2-set of x belongs/does not belong to  /  . The density of  in the  —  part is too small, so elsewhere it must be too big. That gives us a density increase for  on a 12-set and we can iterate. All that can go wrong is if  does not have near maximal density in  . But that would have to happen with positive probability, which means that somewhere we would get a density increase. Metacomment. At the moment, as is clear, I haven’t quite got to the point of plunging in and writing things up formally, but the formality is increasing and I’m still not getting any sense that it’s on the point of collapse. This raises questions about how we should proceed if we get to the writing-up stage. The wiki has been great for that, but I think the inconvenience of it would start to bite quite hard if one were actually writing things up in complete detail. (Unfortunately, Luca hasn’t yet written a LaTeX2wiki converter …) What I’d ideally like is to start writing things in LaTeX but in such a way that others can edit it. I’m not sure if that is technically feasible, however. Another possibility would be to write a skeleton version on the wiki, with statements of the main lemmas and things like that, and then work on the proofs in LaTeX. Or I could just go ahead and do what Ryan did and post a link to a pdf, which anybody could comment on, and which I’d happily send to anyone if they wanted to add to it or make changes (though we would then have to know at all times who was in charge of the latest version). Or we could do something like that but split it into a number of subfiles, one for each section. 870. Metacomment Perhaps using the wiki for LaTeX may not be that cumbersome, the wiki has the great advantage of being a common repository already, plus it has the tools to compare changes between versions. The idea would be to use the wiki simply as a storing device for the latest common LaTeX version, not a place to edit or view the paper. Each time one user A would like to read and/or change something in the LaTeX (say one section at a time as you suggest), A would simply copy-paste the latest wiki version of that section into a blank LaTeX template file on A’s computer and continue editing and monitoring the pdf locally. When done A would simply: (1) copy-paste that new LaTeX section onto the current wiki version; (2) check whether some changes to that section by some other user B have been made during A’s local editing. If not, then no problem: A’s version is now the common current one and appears as such on the wiki. On the other hand if A sees that a new version by B had appeared in between, then A would quickly edit the wiki of that section and add as first line “merging in progress” (a signal preventing disciplined other users to make further changes). Then A and B would need to dicuss their respective version in the wiki discussion page or a blog thread, reach a satisfactory common one, add it to the wiki, and finally remove the “merging in progress” tag. Since only a few people are working on the project this situation should be fairly rare and localized, so that independent parts of the common overall LaTeX file would progress quickly. Archived versions of the whole pdf file, say one per day, might be storable on a separate blog thread for example.\n",
            "3.                         hope  easy  guess   means   most    done   things     fact           somewhat        easier  say          lets                     lets                     let                           easy     done           fact  think  need  give  quick proof         use     result  say          expected              depends             saying              possible                       think theres  doubt           lets suppose   let        let          must                  similarly          true       let            use     similarly                         really       lets fix            lets       similarly                 most              therefore                   most            most    lets suppose             will denote           according  whether       belong                      must     gives                  wrong               would        means    would get          clear    got         things  formally          getting  sense          questions    should proceed   get         great     think     would start    hard    actually  things     unfortunately   yet       id     start  things               sure     feasible however  possibility would           statements      things     work        could                  could     id    anyone   wanted          would    know              could  something    split             perhaps using     may         great       already     tools       idea would   use   simply                 view        would   read   something    say        suggest  would simply                           done  would simply              whether             made         problem          appears                    appeared     would quickly              progress               would need    respective     discussion      reach  satisfactory              progress        working      should  fairly           overall   would progress quickly     whole   say    might         example\n",
            "4. In the first line of page 53 (i.e. the line above the inequality for  ), it should be  (instead of  ) – since  appears in the denominator of the expression in the preceding line (last line of page 52.) [Corrected, thanks – T.]\n",
            "4.        ie          should   instead       appears                   \n",
            "5. Dear Nicolai, This is very interesting! Pleas do elaborate on the various general constructions giving d(n-1)+1. The argument that it is tight for families which contains all elements is also interesting. Can’t you get even better constructions for multisets based on the ideas from your paper?\n",
            "5. dear     interesting       general  giving   argument            also interesting cant  get  better    based   ideas   \n",
            "6. Being new to the project I first say hello to everybody and then deal with the next case. In the context of Kristal’s post I assume f(2)=-1, f(5)=-1. For the sake of contradiction I assume furthermore f(3) = 1. Then Kristal showed that f(7) has to be -1. With these assumptions, the partial sum f[18,21]:=f(18)+f(19)+f(20)+f(21) equals -3+f(19). Since f[1,odd] is in {-1,0,1} it follows that f[18,21]>=-2 and thus f(19)=1. Similarly we get: f[168,171]=3+f(17) f(17)=-1, f[34,37]=3+f(37) f(37)=-1 and f[74,77]=3-f(11) f(11)=1. Since f(9)=f(10)=f(11)=f(12)=1 it follows that f(13)=-1. f[50,55]=-5+f(53)>=-2 which cannot be satisfied for f(53) in {-1,1}. Hence f(7) cannot be -1 and thus f(3)=-1. It remains to show (by hand) that there is no completely multiplicative sequence with discrepancy 2 and f(2)=f(3)=f(5)=-1.\n",
            "6.        say          case   context     assume       contradiction  assume                assumptions             follows    thus  similarly  get           follows     cannot       hence  cannot    thus     show       completely       \n",
            "7. The restriction to the simplex is needed to force the products  and  to be bounded by  (see second display before (11)), which is in turn needed in order to have good equidistribution estimates available (with error term smaller than the main term).\n",
            "7.       needed             see         needed                \n",
            "8. In the light of that I want to make doubly clear that the main point of my previous question was not whether that particular idea would give a suitable damping mechanism (though I did have my hopes) but rather whether there is some way of getting rid of the oscillations. Alec’s remark about the Liouville function is interesting in this respect, since I’m getting the impression that we need to have a better understanding of why the oscillations occur if we want to make intelligent guesses about how to get rid of them. A general idea that could perhaps underlie an algorithm is that if the partial sums are on their way down anyway then there is no need to help them along. That to me suggests an algorithm that looks ahead a bit. For instance, if you’ve chosen the values up to  , then perhaps you could calculate the partial sum up to 2p, ignoring the places where  is not yet defined, and choose  accordingly. Here’s an idea that probably won’t work, but I can at least explain the motivation behind it. It’s a different sort of greedy algorithm that keeps the partial sums as small as possible but insists that they are all positive . The way it does this is as follows. It starts by setting  for all primes  . It then looks at 2 and sees whether it can change  while keeping all partial sums positive. Finding that it can, it changes  to -1. It then can’t change  (or the sum up to 3 would be -1) so it doesn’t. It can change  to -1 so it does that. It seems possible that this could result in the function  , in which case it works but doesn’t give anything interestingly new. If that happens, then one could make a few “wrong” choices to start with. The reason for doing it is that if the algorithm knows that it mustn’t go below zero, then it will be very careful to slow things down if the partial sums seem to be decreasing to zero too fast. In other words, it might lead to a natural damping tendency.\n",
            "8.       want    clear        question   whether  particular idea would give          hopes  rather whether      getting      remark      interesting   respect   getting     need    better         want   intelligent guesses    get     general idea  could perhaps                    need  help      suggests                    perhaps  could               yet defined       idea  probably wont work      explain  motivation     different sort            possible                follows                  whether           finding           cant         would                 seems possible   could result        case  works   give anything        could    wrong   start   reason                  will   careful   things      seem          words  might    natural  \n",
            "9. This problem is sort of cute but it looks a better idea would be to discuss it first as “an open problem of the week” and get feedback from people in analytic number theory (and computational complexity if we allow some assumptions regarding derandomization.) Or discuss it privately with more experts in ANT/CC. The advantage of this problem is that it is a sort of challenge to researcher in one area (analytic number theory) to respond to questions which naturally (well, sort of) arises in another area (you may claim that finding large primes was always on the table but this specific derandomization and computational complexity formulation wasn’t). OK you can claim that if the challange to derandomize the primality algorithm was posed to number theorists early enough (say in 1940), this could have led to the results that the AKS primality testing was based on. But reversing the table seems like a long shot.\n",
            "9.  problem  sort       better idea would   discuss      problem     get       theory        assumptions    discuss            problem      sort          theory  respond  questions    sort       may claim  finding    always      specific       ok   claim                enough say    could  led   results       based      seems    \n",
            "10. Thanks Jaan! Please give us the coordinates of all 17 centres (in quartet notation if you like). I see 17 vertices in the figure, but you say that not all centres are shown – ? Also you seem to say later on that you are only using the original six copies – I am confused.\n",
            "10.   please give               see        say         also  seem  say       using  original      confused\n",
            "\n",
            "Topic: 2\n",
            "1. Is there a nice extension of Rota’s conjecture when instead of  sets of size  you are given  sets of size  where  can be also smaller or larger than  ?\n",
            "1.    nice      instead                   also      \n",
            "2. To clarify: I didn’t mean that one would just shift an entire row but keep it the same shape as the other row that uses the same colours. The distance between (and indeed the shapes of) the centres of the red and purple tiles in the upper row would not be constrained to be the same as for the lower row; similarly for the yellow/blue/green rows.\n",
            "2.  clarify   mean   would    entire                     indeed                would             similarly    \n",
            "3. In the writeup, in the line below (11), it seems that “zero” should be “nonzero at the origin” (as in the wiki sub-page on  zeros dynamics.)\n",
            "3.          seems   should              \n",
            "4. Thanks. I will start working on this. In an attempt to write neater formulas, I made a few more typos.  and  in the numerators and denominators are different. For minimal change, the denominator ones could be written as  and  (with  and  and  ) [Corrected, thanks – T.] Also, while RH verification is a major project on its own, I was discussing with Rudolph earlier in the day that the barrier method, by creating a link between the verified height of zeta zeroes and the dbn bound would allow such a future project to lower the bound further (and also hopefully tighten the density estimates of the off-critical line zeta zeroes which would be a major motivation to do it, but the latter has been asked earlier by other commenters in different forms and probably is quite difficult to formalize). I had referred to the 10^12 number due to Gourdon et al (although it’s not clear whether the large heights in their work were independently verified).\n",
            "4.   will start working     attempt      made              different       could                  also             discussing          method                would            also hopefully            would    motivation         asked      different   probably   difficult               although   clear whether      work   \n",
            "5. Well, the NP-oracle algorithm is obvious: once we know that  contains a prime for some term  of the language, we can repeatedly bisect this interval and query the oracle to see which side contains a prime. Since for every term  there is a standard  such that  , this homes in on a prime in polynomial time. So I would conjecture that some weak Bertrand’s Postulate is provable in  , namely that there is a term  such that  proves that there is always a prime in the interval  . This term could be much larger than  , leaving room for weaker forms of PHP or some completely different idea… The PLS possibility is intriguing. The dynamical system attack suggested by Tao may be on the right track (though we may want to increase the number of neighbors a bit). Note that the PLS algorithm doesn’t need to produce the prime itself, only some datum that can be decoded into a suitable prime in polytime, so that gives a bit more room to set up good cost functions. I don’t have definite thoughts, but this looks plausible. I’m afraid I still have no intuition for the polytime possibility…\n",
            "5.      obvious   know                        see                               would            namely          proves    always          could                completely different idea   possibility      attack suggested   may    right    may want               need                     gives              definite thoughts    plausible       intuition    possibility\n",
            "6. Here’s a very small extension of the plot for the algorithm that’s constrained to remain positive I showed before . With my computer skills I’m not going to be able to extend this much further – I could probably leave my laptop working away for 24 hours and not get anywhere near the plot Alec could generate in an hour! (It would also be good to have an independent confirmation that the code which generated this plot is correct, since I’m somewhat less confident in it…)\n",
            "6.                              able  extend      could probably    working       get anywhere     could      would also                correct   somewhat  confident  \n",
            "7. OK, I computed that  works with this choice, but it would be good to get some confirmation.  is relatively small here (about  ) but this is still enough to have some impact; without  one could lower  to  . So it looks like it is time to work on bounding  again!\n",
            "7. ok     works      would    get     relatively          enough     without   could              work    \n",
            "8. First line of the abstract: Remove either “  ” or “  denote the quantity”. [Changed, thanks – T.]\n",
            "8.       either       denote      \n",
            "9. The only reason  is required to be a bounded set is that so one can interpret the product  of all the primes in  as a finite integer (as opposed to a profinite integer or adele or something more exotic). This restriction also appears in Definition 2.7 and Definition 2.21. One could of course restrict to subsets of [1,Q] without loss of generality (and for our application one can take [w,Q] for instance); actually, now that I think about it, the Chinese remainder theorem shows that the I = [1,Q] case implies all the other cases. So maybe a remark after the claim on these points would be in order.\n",
            "9.   reason   required                                  something     also appears  definition   definition   could        without   generality       take    actually    think       shows      case implies    cases  maybe  remark after  claim    would   \n",
            "10. 1258. Lower bound for density of combinatorial line free k cube of side n From 1170 we get a formula for slice desity based on lower bounds for sets without k term geometric progressions and we get if k is greater than 2^n +1 C\\frac{\\sqrt[2n]{log N}}{2^{n2^{(n-1)/2}\\sqrt[2n]{log N}}} Now if we restrict ourselves to several standard deviations from the mean then all the slices will be roughly the same size as the limit of (1+1/c(n^.5))^c(n^.5) is roughly e^-c the density is roughly the same there will be a factor of 1/2^5 on the constant multiplying the exponent 2^c(log n)^.5 also in the original I think N had to be k*n+1 since the values for which the arithmetic series are excluded go from 0 to kn and here that range would be roughly c(n^.5)\n",
            "10.                  get      based      without       get                      mean     will  roughly          roughly     roughly    will              also   original  think                         would  roughly \n",
            "\n",
            "Topic: 3\n",
            "1. Now for the “related problems” involving linear seminorms, which I have compiled together with Siddhartha Gadgil. It turns out that a majority of these are fairly simple observations to make, so we just mention a few remarks. (1) The first involves the “main question”. There are two variants: (a) Does there exist a non-abelian group G with a linear norm (i.e., that is positive outside the identity)? (b) Does every linear seminorm of a group G factor through its (torsion-free) abelianization? Note that (a) and (b) are equivalent, as the norm-zero elements form a normal subgroup of G. However, (a) and (b) are not equivalent when restricted to a specific group G. The main theorem above resolves these questions for every group G. —- (2) The original theorem was a “rigidity” result (see (1)(b)). However, if we weaken any of the conditions, then either examples exist or the condition turns out to be equivalent to the original problem: (a) If we replace “groups” by “monoids”, then examples indeed exist. Consider [via Robert Young] the free monoid on any alphabet, with the (edit) distance between strings v,w being the least number of single generator insertions and deletions to get from v to w. The triangle inequality and positivity are standard; linearity is trivial; and the bi-invariance  follows by considering  {generator, identity}. (b) If we replace “linear growth” by “almost linear growth”, then Tobias Hartnick’s two comments show existence. (c) The other case of “almost linear” was  for all  — but this is clearly linear, by Sean Eberhard . (d) David Speyer asked a different question , about which Sean Eberhard has some thoughts. Can this be answered? (e) Although there is no linear norm on a non-abeliam group, it can indeed be defined on finite diameter balls in  . See the discussion/comments following this analysis by Terry . (f) The final case is when “seminorm” is replaced by “quasi-seminorm with additive constant c” — as asked by Siddhartha Gadhil . He mentioned to me that he will post more details on this variant, and has done so just a few minutes ago (following this post).\n",
            "1.    related problems        together             fairly simple observations      mention          question        exist         ie                                      however           specific         questions        original     result see  however       conditions  either examples exist   condition        original problem         examples indeed exist consider                            get              trivial     follows  considering           almost        show existence    case  almost           clearly         asked  different question        thoughts    answered  although            indeed  defined        see   following         case              asked           will         done       following  \n",
            "2. I don’t have any deep insights but my first thought was to explore some randomly generated examples. I wrote the following code in a few minutes so its probably embarrassingly inefficient but perhaps someone else could improve it. It outputs random union-closed sets that have a low abundance. I will look into better ways of visualizing the output. https://sagecell.sagemath.org/?q=xkqack It would be very nice to have a library of small examples we could use (and put on a Wiki page) in order to test conjectures. In a very similar vein, here is a Mathematica Manipulate that randomly generates union closed families, then plots the frequency distribution of the elements of the original set (after a rename/sort) within the family. It was interesting to see as the family size got larger, all elements of the original set seem to occur in just over n/2 sets within the family. Manipulate[ X = Table[i, {i, 1, m, 1}]; powerSet = Subsets[X]; psLength = Length[powerSet]; randomSample = RandomSample[powerSet, r]; pairs = Subsets[randomSample, {2}]; munion = DeleteDuplicates[ Map[Apply[Union, #] &, pairs]]; mcounts = Sort[BinCounts[Flatten[munion], {1, m + 1, 1}]]; Column[{ Row[{“Set is “, X}], (*Row[{“PowerSet is “, powerSet}],*) (*Row[{“RandomSample of PowerSet is “, randomSample}],*) (*Row[{“Pairs are “,pairs}],*) Row[{“A union closed Family of Subsets with Cardinality “, Length[munion], ” is “, munion}], Row[{“Counts are “, mcounts}], (*Row[{“Histogram is “, Histogram[Flatten[munion],{1,9,1}, ImageSize\\[Rule]{400,200}, GridLines\\[Rule]{{},{Length[munion]/2, Length[munion]}}] }],*) Row[{“Ordered freqs are “, ListPlot[mcounts, Filling -> Axis, ImageSize -> {400, 200}, PlotRange -> {0, Length[munion]}, GridLines -> {{}, {Length[munion], Length[munion]/2}}] }] }] , {{m, 9, “Original Set Cardinality”}, 0, 19, 1} , {{r, 3, “Random Sample Size”}, 0, 1000, 1}] Is it easy to describe how your random generation works? Do you pick a few random sets and take the union closure? Yes, that’s about it. I create the power set, take a random sample from the power set, then create the closure by taking the union of all pairs formed from the cartesian product of the sample with itself. The DeleteDuplicates, above, creates the family from this list of unions. Not the most efficient way, I’m sure, and I notice that the null set is never in the closure, also. A slider in the Manipulate allows the size of the random sample to be changed interactively. Rik, The families created aren’t necessarily union-closed. The code only takes unions of _pairs_ of sampled sets. (It also discards the original sample.) The following seems to work correctly. For efficiency, I’ve used bistrings (binary integers) to represent the sets. Manipulate[ randomSampleInts = RandomInteger[2^m – 1, r]; ucFamilyInts = Nest[DeleteDuplicates[BitOr @@@ Subsets[#, {1, 2}]] &, randomSampleInts, IntegerLength[r, 2]]; ucFamilyBits = IntegerDigits[#, 2, m] & /@ ucFamilyInts; elementCounts = Total@ucFamilyBits; Column[{ Row[{\"Base set is \", Range@m}], Row[{\"Cardinality of union closed family of subsets is \", Length[ucFamilyInts]}], Row[{\"Counts are \", elementCounts}], Row[{\"Ordered freqs are \", ListPlot[elementCounts, Filling -> Axis, ImageSize -> {400, 200}, PlotRange -> {0, Length[ucFamilyInts]}, GridLines -> {{}, {Length[ucFamilyInts], Length[ucFamilyInts]/2}}]}], Row[{\"Union closed family is \", Sort[Join @@ Position[#, 1] & /@ ucFamilyBits]}] }] , {{m, 9, \"Base set cardinality\"}, 0, 19, 1} , {{r, 5, \"Random sample size\"}, 0, 100, 1}] Ah yes, thanks for the correction, and the far more efficient way of tackling the calculation. (Also thanks for the Wolfram Language lesson, most instructive.)\n",
            "2.          thought   explore    examples    following        probably    perhaps someone  could improve             will look  better        would   nice       examples  could use               similar                       original  after        interesting  see     got      original  seem                                                                                                                      original                 easy  describe     works   pick      take    yes         take                                          most efficient   sure         never    also                      necessarily             also   original   following seems  work correctly                                                                                         position                         yes    correction     efficient      also      lesson most \n",
            "3. Just to check, should that last  be  ?\n",
            "3.    should      \n",
            "4. Let me try to show the idea in a different setting where it might be more productive. Consider now non-uniform sequence of length  on  elements, and let’s look at the supports  and  , where  is some number, potentially depending on  . If either of the two supports has size less than or equal to  we can conclude that where are at most  elements on one side of it, and, therefore  . On the other hand, if both supports have size greater than  then they have an element in common and thus  . Now, if we choose  to be a constant than the first estimate is polynomial and the second one is exponential which is not interesting. But what if one sets  ? Then the second estimate is linear and the first one gives something like  , which is much more interesting but still nothing new. There must be a yet better choice for  .\n",
            "4. let  try  show  idea   different    might    consider           lets look             potentially depending     either               conclude     most         therefore                       thus                           interesting                  gives something        interesting   nothing   must   yet better    \n",
            "5. In theorem 2.14, the condition  can be relaxed to  . (Indeed, in case (i) of my comment above, the optimal  is  . [Corrected, thanks – T.]\n",
            "5.     condition        indeed  case               \n",
            "6. IMO comment 14 is not a complete solution. It may well be correct, but there is a gap. There is an assumption, perhaps using spatial intuition, that when the line has rotated by pi it will have returned to the original, central pivot. This may be true but it requires a proof. The pivot point changes and can move back and forth to points on both sides as well as the central point. How do we know that, after the rotation by pi, it hasn’t moved so far to the left or right that the central point can’t be the pivot? I have struggled to find a proof, or at least some idea pointing in that direction, but I haven’t managed it.\n",
            "6.          may   correct         assumption perhaps using  intuition          will     original    may  true   requires  proof        back                 know  after              right     cant        find  proof     idea pointing   direction     \n",
            "7. No: APs that begin at zero.\n",
            "7.      \n",
            "8. Here is another idea for a randomized construction. Instead of constructing the  independently for each prime and forming intersections, can we do a kind of randomized sieve of Eratosthenes to generate ‘random primes’? That is, let  ; given  , let  be the smallest number not in any of them, and let  consist of  together with a random selection of other numbers each with probability  . I’ve just found out that this is known as the Hawkins random sieve, and an analogue of the Riemann hypothesis holds for it, as well as lots of other nice facts: see for example this paper . We could then proceed to define  as above, as subsets of  , and then form intersections; or there may be some more general sieve-like construction to get general HAP analogues more directly. Yet another, very simple, idea would be simply to define  , for any  , by  , and take the initial segments of the  as the HAP analogues.\n",
            "8.    idea    construction instead                kind            let      let            let     together               found     known            hypothesis       lots   nice facts see  example     could  proceed  define               may    general  construction  get general    directly yet   simple idea would  simply  define           take          \n",
            "9. 484162\n",
            "9. \n",
            "10. Thanks! (BTW, I meant to identify ALL the convenient arbitrary parameters used throughout the analysis and their necessary constraints in order to replace the 207, 43, 1/4 by these “control” parameters to optimize  .\n",
            "10.    meant      arbitrary        necessary                 \n",
            "\n",
            "Topic: 4\n",
            "1. That sounds like a very interesting line of enquiry, which could lead to problems that were more tractable. However, I would expect the behaviour of this random model to be importantly different from the behaviour of the original problem. For example, in this case we would be greatly smoothing the number of “factors” of each integer n, whereas the original problem seems to be importantly affected by the fact that there are lots of primes, where we are relatively free to make choices. Of course, the primes get much sparser later on, but there is still much more variability in the number of factors of a random integer than there would be with these pseudofactors. Then again, perhaps one could incorporate that into the model. Probably quite a lot is known about the distribution of the number of factors of n. (I mean factors rather than prime factors, so I’m not talking about the Erdős-Kac theorem here.) So perhaps one could let i affect n with a probability p(n), where 1/p(n) is distributed in the same way as the number of factors of integers around n.\n",
            "1.      interesting     could   problems     however  would expect          different      original problem  example   case  would              original problem seems       fact    lots      relatively         get                       would       perhaps  could      probably     known            mean  rather       talking       perhaps  could let                        \n",
            "2. My  is not the same as  in Lemma 5. My  (perhaps I should have called it  ) is a convenient variable for defining the counterexample, and the example is ok (you can check by hand that it is neither of the three types in Lemma 5). About Zhang’s technique: the condition in Lemma 5 is a certain natural limit, as shown by my example, but hopefully this limit is not a fundamental one.\n",
            "2.              perhaps  should         variable    counterexample   example  ok                   technique  condition      certain natural      example  hopefully      fundamental \n",
            "3. 230. @sw (201). Regarding the graph approach, it’s true that Menger’s theorem does not seem to apply because you need to take the graph directed. But in fact we don’t need Menger’s theorem at all. If there are n vertex independent paths between the “0 vertex” and the “total sum vertex”, obviously there is no way to disconnect them taking out n-1 vertices (each vertex you take out will belong to at most one path). What Menger’s theorem would say (if applicable) is that the “n disconnecting vertices” statement and the “n vertex independent paths” statement are equivalent. Without Menger’s theorem, the “n vertex independent paths” statement could happen to be wrong… Anyway, this still seems worth thinking to me.\n",
            "3.       approach  true      seem  apply   need  take      fact   need                  total   obviously               take  will belong   most      would say         statement       statement   without        statement could    wrong    seems worth thinking  \n",
            "4. Ok, i suppose I started with 1 (not with 0) it makes no difference if you insist your sets will be from the same slice with the two definitions together. But starting with ‘1’ will not allow even the coarse slices to have lines. Now the individual slices in the index sum definitions are smaller. (Actually a single slice in the ususal sense is at most  and not  as the wiki suggests, right?) Each slice in the new sense have  (when you just take the sum of indices) and if you insist on a slice both in terms of the sums of indices and in terms of number of coordinates of each type (and then you can start with 0) you get slices of at most  . Now we want to gather many such slices to avoid a combinatorial line. So it looks that if the indices of the slices are (a,b,c,x,y,z,) we should not include 6-tuples avoiding 3 term a. p. simultaniusly in both b+2c and x+2y The tragedy is that it seems that unions of such slices will give you examples in the same ball park as the examples we already have. but I am not sure about it. Also if it is in the same ball park asymptotically it can lead to better examples than the hyper optimistic ones. We may be able to explore the situation for union of slices in the different sense and in the refined sense for small but not terribly small n’s. the question is related to the maximum sizes of subsets of a rectangle {1,2,…,a} times {1,2,…,b} not having a 3-term A.P. I suppose the asnwer is in the same ball park as for {1,2,…,ab} but I am not sure about it and I suppose experts would know.\n",
            "4. ok  suppose         makes        will          together  starting   will                     actually       sense   most        suggests right      sense      take                                  start    get    most     want                        should                    seems      will give  examples        examples  already      sure   also              better examples    optimistic   may  able  explore         different sense     sense         question  related                   suppose                sure     suppose  would know\n",
            "5. In the process of filling in the table, I found a significantly better tuple for Pintz’s k0=181,000, with diameter 2,326,476. http://math.mit.edu/~drew/admissible_181000_2326476.txt\n",
            "5.          found   better        \n",
            "6. I think it is the Graham-Leeb-Rothschild theorem. Here is the corrected version of the paper in which it is proved: http://www.math.ucsd.edu/~sbutler/ron/87_10_categories.pdf .\n",
            "6.  think                  proved  \n",
            "7. I just realized something silly. The switching of signs, the decreasing of absolute values, and the pair above looking like  are all artifacts of Mathematica normalizing the optimal  . Recall that  is really only determined up to a constant. What we should have been doing this whole time is normalizing so that the constant term is exactly 1. Sorry for not realizing this earlier.\n",
            "7.   realized something silly        absolute      looking                 really         should     whole          exactly  sorry   realizing  \n",
            "8. Tim asked me to write something about online versions of Rota’s Basis conjecture. Here, the n bases arrive one at a time, and after you receive the i-th basis, you have to decide immediately in what order to write it as i-th row of the array you are creating. Of course, the goal is that the columns of the eventual array are again independent. Is there an algorithm that does this for i up to and including n? There are at least two versions of this question: (1) The input consists of vectors in a vector space. In this case, for _even_ n, the online version still follows from Alon-Tarsi (in char 0), and for _odd_ n any online algorithm can be fooled in the last steps (if the field contains sufficiently many roots of unity); see https://arxiv.org/pdf/1312.5953.pdf (2) The input consists of elements in an abstract matroid. In other words, when the new basis arrives, you get told which subsets of the elements so far are independent, but not more. In this case, an online algorithm can be fooled regardless for any n≥3 (even with linearly realisable matroids); see page 9 of Click to access 1312.5953.pdf This leads to a question that I think might prove tractable: determine the maximal number f(n)<n (n≥3) such that there exists an online algorithm in scenario (2) for arranging f(n) bases, but not for arranging f(n)+1 bases. E.g., does f(n) grow as a square root of n?\n",
            "8.  asked    something      basis            after     basis                        goal                                  question             case        follows                             see              words    basis   get                case                see         leads   question   think might prove  determine                          eg         \n",
            "9. It would be great if Tim could write up the proof. I’m trying to write a toy version of the proof for corners on the Cartesian product AxA, where A is a Hilbert cube. A few weeks ago my first try didn’t go well as I followed the original Ajtai-Szemeredi proof. I knew it quite well and didn’t pay attention to Tim’s modified proof. Now I see that actually the key of the DHJ3 proof is hiding there. So, I advise the interested reader to compare Tim’s version to the original, even if one knows the Ajtai-Szemeredi proof very well. The two wiki entries are: “Ajtai-Szemerédi’s proof of the corners theorem” http://michaelnielsen.org/polymath1/index.php?title=Ajtai-Szemer%C3%A9di%27s_proof_of_the_corners_theorem and “A Modification of the Ajtai-Szemerédi argument” http://michaelnielsen.org/polymath1/index.php?title=Modification_of_the_Ajtai-Szemer%C3%A9di_argument\n",
            "9.  would  great   could    proof  trying        proof                    try        original  proof         attention    proof   see  actually  key    proof        interested        original       proof         proof            argument \n",
            "10. The error was in an insignificant constraint. Internal constraints (not greater than 1): tile 5: 258-156, 245-156, 245-568, tile 8: 128-368, 128-568, 178-368, 178-568, 178-258, 568-478, tile 6: 568-236, 368-276, 568-267, The first four and the last one are exactly 1. External constraints (not less than d): tile 8: 38-128, 178-568, tile 5: 15-258, 156-245, tile 6: 568-167, 368-167 The first three are exactly d.\n",
            "10.                                        exactly                        exactly \n",
            "\n",
            "Topic: 5\n",
            "1. Ah, excellent – thanks – I didn’t realise it was for d=1. I get 2.0055 as the best M for k=4 and d=1 (the epsilon being 0.16841). Does the fact that your value of 2.01869 is slightly bigger imply that there is still some unexplored space here? – for example, scope to increase M using partitioning, or non-polynomials? (0.003 isn’t much, but I guess there might be a greater disparity for larger k and/or d.)\n",
            "1.              get    best            fact         imply           example     using         guess  might         \n",
            "2. Suppose we decide that we want to deal with the EDP for multiplicative functions (taking values in {-1,1}). There seem to be two sorts of examples with low discrepancy. On the one hand there are functions that behave rather like random  sequences, such as the Liouville function. On the other hand, there are functions that are character-like, where you choose  and the value of  , and for all other primes  you set  if  is a quadratic residue mod p and -1 otherwise. (Let me try to render the Legendre symbol:  .) These two types of example behave very differently, which makes me wonder whether in order to prove a general result about multiplicative sequences one might need to classify them somehow. In an ultra-ideal world, one might be able to say that either the function is “disordered”, in which case you get the typical square-root growth (or at least pretty fast growth), or it is “ordered”, in which case it has to be somewhat character-like, which allows one to analyse it more explicitly. The question one then faces is how to do this classification. As a very small step in the right direction, I want to enlarge slightly the class of functions that deserves to be called character-like. The motivation for this came from thinking about what happens to a character-like function if you change it at just one prime. At first I thought that this would mess up the function completely, but I now see that that is not the case at all. Let us write  for the completely multiplicative function that takes  to 1 and  to  when  . Let’s also write  for the same thing but with  going to -1. Now let’s suppose that we take either  or  and change it by altering the value at one particular prime  and leaving the values at all other primes unchanged. What effect does this have on the function and its partial sums? First, the effect on the function. What we are doing is multiplying it (pointwise) by the completely multiplicative function  that takes the value -1 at q and 1 at all other primes. This function deserves to be called character-like as well: it’s just that at numbers coprime to  we take the trivial character rather than the Legendre character. Since it is has a strong periodicity property (that is, it is periodic except at multiples of  , which themselves are periodic except … etc.), when we multiply  by  or  we should get a function that has some kind of periodicity property with period  . And indeed we do. On all numbers coprime to  we get periodicity (with period  ), and more generally if we fix  and  and look at all numbers of the form  such that  , we get a sequence (with gaps) that looks just like the sequence along numbers coprime to  . (Or rather, we get plus or minus that sequence.) These sets partition  , and at most  of them intersect the set  . This tells us that the partial sums grow at worst like  , or in other words proportional to  , with a constant of proportionality that one expects to go up like  or so. More generally, if we do things like multiplying  by  , we will get other functions that have pretty good log-squared type behaviour. So we do not have a straight dichotomy between log on the one hand and square root on the other. I have not tried to find a deep connection, but I cannot help being reminded of Littlewood’s problem about minimizing the  norm of the Fourier transform of the characteristic function of a set of integers of size  . There, it turns out (highly non-trivially) that the smallest possible norm is  , achieved when the set is an arithmetic progression. As for “random” sets (under various sensible notions of randomness), they give square-root-like behaviour. But again there isn’t a log/root dichotomy because you can take two-dimensional progressions (which from a certain point of view — I won’t go into it right now but some people reading this will know what I mean — are rather similar to products of two one-dimensional progressions) and get bounds of  . What is known about Littlewood’s problem is actually slightly stronger. Suppose you have a real-valued (actually, maybe it can be complex-valued as well) function  and a set  of integers of size  , such that  for every  and  for every  . Then  . So there’s a wild idea: can anyone take a completely multiplicative function and build out of it a function that’s large on some set and zero outside that set, and relate the partial sums of the first function to the  norm of the Fourier transform of the second? I have one small additional remark to follow on from my previous comment, which is that if you take a character-like function such as  or  and multiply it by a product  for a sufficiently sparse set  of primes, then you can get slow growth rates that are nevertheless faster than any power of  . The idea is that you start with  itself, which gives you logarithmic growth. You let it chug along for a while, but you then multiply by  for some large prime  . At this point the growth becomes proportional to  . You then let that chug on for a while (a very long while if you want) before multiplying by  for some  that’s much bigger than  , and so on.\n",
            "2. suppose     want              seem    sorts  examples             rather                                                 otherwise let  try            example     makes  wonder whether    prove  general result     might need    somehow      might  able  say  either       case  get        pretty         case     somewhat         explicitly  question                 right direction  want               motivation     thinking                   thought   would     completely    see      case   let      completely               lets also              lets suppose   take either             particular                                        completely                                   take  trivial  rather                  except          except  etc          should get      kind         indeed          get        generally   fix     look             get                  rather  get             most                  worst       words                     generally    things        will get     pretty                            tried  find      cannot help     problem                                possible     achieved                 randomness  give             take      certain   view   wont    right       will know   mean   rather similar        get       known   problem  actually  stronger suppose     actually maybe                                   theres   idea  anyone take  completely    build                                         remark  follow           take                           get               idea    start     gives     let                               let             want                \n",
            "3. Here’s the intuitive, but very possibly wrong, reason that it seems to me not to be possible to find a set of matrices of the above form. If each  has low total oscillation, then there ought to be a fairly large rectangle inside which all the  are roughly constant (or rather, roughly constant when both coordinates are multiples of  and zero elsewhere). Now let’s just look at primes and let’s suppose that the  are actually constant matrices. Then the value of the sum of the  at  can be expressed by a formula of the form  . But that expression just doesn’t look as though it can be anything like constant: for instance, if we restrict attention to small primes, then in any large rectangle we can find pairs  and  such that the highest common factor is a product of all small primes for which  is large and positive, and other pairs that are coprime. The weaknesses in that argument are that larger primes do exist, and they make things more complicated, and also that, as I noted earlier, one can impose quite a lot of oscillation on, say, the sequence  , while still having a bounded total oscillation. So the large rectangle doesn’t obviously exist.\n",
            "3.   intuitive   possibly wrong reason   seems      possible  find              total        fairly         roughly   rather roughly             lets  look    lets suppose     actually                              look      anything        attention           find                                   argument      exist    things    also               say          total       obviously exist\n",
            "4. A tiny notational note: as you can see from the slides, I actually went with  . Two reasons I ended up preferring this: a) makes the deduction on Szemeredi from DHJ plainer (“just write numbers in base  “) b) computer scientists like  .\n",
            "4.        see     actually      reasons       makes                    \n",
            "5. I mean circle with unit radius.\n",
            "5.  mean    \n",
            "6. Thank you for sharing that. I think that for FUNC a year may be a bit too long, unless things pick up again. But I now think that the best option for FUNC is probably option 2: that is, write up the discussion so far but in the hope that it stimulates further progress. And then if it doesn’t, it will be easy to close the project and say that the document is its write-up.\n",
            "6.       think      may      unless things pick      think   best option    probably option       discussion      hope     progress       will  easy      say      \n",
            "7. I share your preference for a unified pseudonym, partly because it means that Polymath’s papers will straightforwardly appear in the same place on MathSciNet (without the need to remove the initials), the arXiv etc.\n",
            "7.         partly   means    will  appear       without  need       etc\n",
            "8. To wrap this up, let me explain that 2) is indeed far from true, so that this approach cannot be a general proof strategy. The reason is that there are arbitrarily large finite lattices that are simple , i.e. do not have any nontrivial lattice quotients. For example, there is a theorem of Ore saying that the partition lattice of a finite set is simple. (See Theorem IV.4.2 in Grätzer’s “General Lattice Theory.)\n",
            "8.     let  explain    indeed   true    approach cannot   general proof strategy  reason     arbitrarily      simple  ie     nontrivial    example       saying          simple see     general  theory\n",
            "9. Regarding our very basic problem let me point out that there are two variants: 1) (effective) find a constant A and a deterministic algorithm that find a k-digit prime wose running time is bounded above by  . 2) (non effective) Prove that there is a deterministic time algorithm whose running time is bounded above by  for some  . As far as I can see Cramer’s conjecture gives only a non effective algorithm (version 2).\n",
            "9.    basic problem let           find         find                prove                         see   gives       \n",
            "10. Question for academic community: Best Regard, Tao and community Hi! I am a Yeisson Acevedo, currently I am master student in applied mathematics at the Eafit University (Colombia), I am interested in number theory and I would like to ask you a question that has to do with my object of study that I do not know if it is relevant or not to publish a possible paper, and i would like to ask the favor if you give me your opinions about the next questions: 1) is there a Mersenne Prime such that  for some Integer k? 2) Is there a Mersenne prime who is at the same time Germain’s prime? 3) Is there a Mersenne prime who is at the same time Fermat’s prime? 4) Is there a Mersenne prime who is at the same time Wagstaff prime? 5) Is there a Mersenne prime who is at the same time Wodall prime? 6) Is it possible to estimate if around a Mersenne prime there are close primes? That is to say, is Mp +- 2 a prime number?, is Mp +- 4 a prime number? We want to know if they are important or not in the academic field of number theory and please excuse us for making them spend their time. Thank you. Pdt: For us, from here in Colombia, the results shared and commented in this blog have been very useful to think about several application problems that go hand in hand with the theory of numbers, although it is not our object of direct study if we use many of your results, so thank you very much for everything. Knowledge is built in community.\n",
            "10. question    best                 applied         interested   theory   would      question              know    relevant      possible    would        give   opinions    questions                                                                         possible               say                want  know    important         theory  please    making               results          useful  think    problems        theory   although           use    results            \n",
            "\n",
            "Topic: 6\n",
            "1. Thanks! Yes, I had not seen that definition. OK, so then the nice Edelsbrunner construction is still 2-degenerate because we can just iteratively clip off the last vertex (c_3, then b_3, etc). Too bad. But it’s definitely a tempting starting-point for seeking something that does work.\n",
            "1.  yes      definition ok    nice  construction                 etc  bad   definitely      something   work\n",
            "2. PS> As you can see in Sections 2,3 –> I refer to an as-yet-unstated theorem from the Introduction. I’m currently inclined for us to state the theorem proved on the blog in the Introduction, and then say that we’ll show the more general quantitative estimate for “quasinorms of commutators” in the next section, before exploring variants in Section 3.\n",
            "2.     see                    state   proved         say   show   general            exploring    \n",
            "3. The mesh exercise for N=151 to 300, y=0.4, t=0.4, c=0.065 is complete. There were around 4.4 mil mesh points. I have kept the file (somewhat large) as a Google drive link here . There are 4 columns -> N, x, |f(x), |f'(x)| (although computing f'(x) wasn’t necessary for the exercise). There is also a much larger file with complex valued f(x) and f'(x) which can be shared if needed. Some interesting rows in the file: N, x, |f(x)|, |f'(x)|, remark 151, 286834.3928, 0.5097, 0.4773, min |f(x)| 152, 291266.7808, 2.9525, 3.7539, max |f(x)| 191, 462997.2589, 0.6210, 0.1150, min |f'(x)| 155, 304719.4104, 2.9371, 3.8945, max |f'(x)| Average |f(x)| and |f'(x)| values were 0.9617 and 0.6104 respectively.\n",
            "3.                         somewhat                  although    necessary      also                needed  interesting         remark                                  respectively\n",
            "4. p54, line -2, “he basic intuition” should be “the …” Purely nitpicking: p55, 4 lines below (4.54), maybe  is better than \"1.19E-4\", etc. Same in Tables 6&7, although I don't feel as bad about them when they are in tables. [Corrected, thanks – T.]\n",
            "4.     basic intuition should    purely       maybe   better   etc     although   feel  bad           \n",
            "5. Here’s another (equivalent) form of the problem whose finite approximations enjoy a bit more symmetry. Let D be a finite set of positive integers (the differences) with least common multiple M, and let N be the set of divisors of M that are also multiples of some element of D (include 0 in N). The discrepancy of a function f (from N to  ) is the maximum of  with the maximum going over all  and all  . Denote this discrepancy by  , and denote the minimum of  over all f by  . Note that  is bounded as D goes through  if and only if Erdos’s question has a negative answer. Understanding  for various D may help us understand how some progressions are interacting. Also, the inclusion of 0 in the discrepancy-defining summation seems to introduce some useful symmetry of the “x goes to  ” sort.\n",
            "5.       problem         let                 let           also                                      denote       denote                          question    answer      may help  understand      also         seems    useful         sort\n",
            "6. I haven’t yet explained the main point of the calculations in the previous comment. They are that if we are searching for a pretty formula, then it makes sense to give some symmetry to the coefficients  And the most obvious way of doing that is to make them independent of  That is, we could search for a decomposition of the more specific kind  Also, we might, in order to give ourselves a bit more flexibility, the opportunity for smooth cutoffs, etc., like to replace the sum over  by a sum with weights. That is, we might like to consider an expression of the form  Again, if we are searching for coefficients that are independent of  we might like to place restrictions on  Note that we are already hoping that  will be concentrated in the set of  such that  , which is the set of  such that  and  are small multiples of  So it feels natural to make  a function of  and  A nice aspect of that is that the integral is also a function of  and  If we do this, then I think the entire expression depends on  only. Let me just check that. Yes, it’s clear, since if we multiply  and  by  we can just substitute  for  and end up with the original expression. I now need to do a little bit of paper calculation in order to work out a nice way of expressing the matrix as a function of  Then one can think about how to make that function 1 at 1 and 0 everywhere else.\n",
            "6.   yet                     pretty    makes sense  give         most obvious               could        specific kind  also  might    give           etc                might   consider                    might           already hoping   will                              feels natural           nice         also             think  entire  depends    let     yes  clear                      original    need    little        work   nice              think            everywhere \n",
            "7. @Andrew, many of the links in your records.txt seem to be broken.\n",
            "7.         seem   \n",
            "8. In the wiki page, the current lower bound for  (prism) should be 2 (due to the corners local maxima.)\n",
            "8.            should        \n",
            "9. For the tables with x ranging from 160 to 3200, by increments of 160, I set y = 0 , and t = 0.4 . For the table that follows, showing a “spike” or discontinuity near x = 804, I think I also set t = 0.4, y=0.\n",
            "9.                              follows showing          think  also     \n",
            "10. Maybe we should settle on something with nicer coordinates, see post above.\n",
            "10. maybe  should   something  nicer  see  \n",
            "\n",
            "Topic: 7\n",
            "1. I also observed that points which are 8/3 apart from each other are forced to the same color when coloring with only four colors. For several days I have tried to find a new record using this observation by 1) picking such a pair, 2) forcing them to have a different color, and 3) reduce the graph using random probing via clausal proof minimization. However, this approach has not been fruitful yet: the smallest graphs with chromatic number 5 based on this are roughly 150 vertices larger than the earlier method. Forcing the points (0,0) and (8/3,0) to the same color can be done without any points that have a y coordinate larger than 1 or lower than -1. This is probably not tight. I will also try to minimize this.\n",
            "1.  also       apart                     tried  find    using  observation             different       using     proof  however  approach     yet        based    roughly       method             done without                 probably    will also try   \n",
            "2. The fact that player A has to choose the number N at the beginning of the game is intriguing. The number of possibilities for x is originally N, so it would seem like large N would make the game harder for B. I suspect that B can counteract the difficulty by asking many more questions for large N than small N.\n",
            "2.  fact                     possibilities        would seem    would    harder    suspect      difficulty  asking   questions      \n",
            "3. On Matroids with No Small Circuits. A paving matroid is one in which the sets of size less than the rank are all independent. With Peter Humphries I proved that: Theorem. Let  be disjoint sets of size  , and let  be rank  paving matroids on  such that  is a basis of  for each  . Then there exist  disjoint transversals  of  such that  is a basis of  for each  . This strengthening of Rota’s Basis Conjecture fails for matroids in general and for  for paving matroids. After an unpleasant case analysis establishing the result for the  case, the result is proved by an easy inductive argument. Tim is proposing extending the results in our paper, for any fixed  , to rank-  matroids in which the  -element sets are all independent. It could well be true that the above theorem itself would hold for such matroids so long as  is large relative to  . To prove such a result one would likely need to do a lot of work to establish it for some value of  , but then the induction should be easy. I don’t see this leading to a full proof of Rota’s Basis Conjecture, but it is a concrete approach toward interesting partial results. On a related note, it is conjectured that the proportion of  -element matroids that are paving tends to one as  goes to infinity. Given the exciting new results of Jorn van der Pol and Rudi Pendavingh on the structure of almost all matroids one might be able to prove that Rota’s Basis Conjecture (for matroids) is almost always true.\n",
            "3.                             proved   let          let              basis         exist            basis           basis      general       after  unpleasant case    result    case  result  proved   easy  argument      results                       could   true      would                prove   result  would likely need      work             induction should  easy   see      proof   basis       approach  interesting  results   related                           results             almost    might  able  prove   basis     almost always true\n",
            "4. Emanuele requested I point out that the lemmas mentioned (which I refer to again below) are not his, just that his paper has pointers to the literature (and a variant of the lemma).\n",
            "4.                                 \n",
            "5. 96. Thread title: obstructions to strong uniformity. Ryan, I think we need to formulate more carefully what it means to have a new obstruction. I would want not to count the example you’ve given because it’s contained a bigger example where  and  consist of all strings (before you do the tacking-on process). And that bigger example is covered by what I called strong obstruction 1 in comment 71.\n",
            "5.          think  need  formulate     means       would want     example        example                  example            \n",
            "6. If I’m not mistaken, you can take Theorem 2.3 (or the more precise Corollary 1) of that paper, and then combine this with Terry’s Fourier Reduction Lemma. This then says that the \\omega(N) lower bound holds for character-like sequences, no? It then remains to treat the non-character-like sequences. Unfortunately, there does not appear to be precise information for this latter case, except that the average value converges to zero. By the way, if anyone can read German, reference [21] of the paper may be useful to decipher.\n",
            "6.       take      precise                 says                   unfortunately    appear   precise information    case except            anyone  read       may  useful  \n",
            "7. Can someone explain the link of 272 to here, from the table with m=1 at: http://michaelnielsen.org/polymath1/index.php?title=Bounded_gaps_between_primes\n",
            "7.  someone explain             \n",
            "8. I’m not even sure how you got the 1/9, I don’t see why these things should multiply. Probably knowing what disks A, B and C are would help. Anyhow, I think that this debate about Bernhard idea is getting too long. The original idea and video seem to lack the rigorousness required from a mathematical proof, and since no one seems to understand it, I think we should wait until Bernhard can post a version that is more mathematically founded, and then we can return to it.\n",
            "8.    sure   got     see   things should  probably         would help anyhow  think   debate   idea  getting    original idea   seem  lack   required    proof     seems  understand   think  should                   \n",
            "9. Let me write  for Kevin’s  . If my calculations are correct, we have:  :  :  : (This last search is still ongoing; I’m not hopeful of getting a definite value by brute force.)\n",
            "9. let            correct                   getting  definite    \n",
            "10. To get to 6-chromatic graphs I would suggest spindling on the point  . This point is easy to find in existing graphs by combining Moser spindals and it introduces the ring generator  . The factor of five eliminates all linear 5-colourings. That may not be enough to eliminate non-linear 5-colourings but it is a start. I don’t think current methods (computational or otherwise) allow us to explore this much further at present.\n",
            "10.  get     would suggest          easy  find  existing                       may   enough         start   think  methods   otherwise    explore     \n",
            "\n",
            "Topic: 8\n",
            "1. 1166. Higher-dimensional Fujimura Jason, how about using  where k is the size of the tuples, or the dimension of the Z^k used? The original Fujimura problem would correpspond to  and the quadruple version to  You get a non-constructive quadratic lower bound for the quadruple problem by taking a random subset of size  . If c is not too large the linearity of expectation shows that the expected number of tetrahedrons in such a set is less than one, and so there must be a set of that size with no tetrahedrons.\n",
            "1.       using                  original  problem would           get         problem                    shows   expected               must         \n",
            "2. Maybe I should spell out why the property I talked about in my previous comment is a quasirandomness property. Suppose you have a system A of k-sets. (NB, A is a collection of k-sets and not an individual k-set. I just don’t feel like typing  the whole time.) Now partition A into B and C. If |B|=r and |C|=s, and if we write E for energy, then we get  where  is the mutual energy , that is, the average size of the intersection of a random set in B with a random set in C. If B and C are reasonably large, and if it is impossible to increase the energy by much if we pass to a large subsystem, then  and  can’t be much bigger than  , from which it follows that all of  ,  and  are approximately equal to  . But then, writing  for the sum of the characteristic functions of the sets in  , and similarly for  and  , we get that  , from which it follows that  and  are approximately proportional to each other, and hence also approximately proportional to  . So we have the following quasirandomness property: the sum of the characteristic functions of the sets belonging to any largish subcollection of  must be approximately proportional to the sum of the characteristic functions of all the sets in  . I will continue this discussion in a subcomment.\n",
            "2. maybe  should                 suppose                       feel     whole                      get                                  reasonably      impossible                  cant          follows          approximately                       similarly       get       follows      approximately      hence also approximately         following                   must  approximately                 will   discussion   \n",
            "3. I now see that Yuri’s comment was not from an earlier discussion thread — I just missed it because I was trying to think more about general constructions than constructions for small \n",
            "3.   see         discussion          trying  think   general      \n",
            "4. Ah yes. I bet we can extend this to unscaleable tilings. Please repost the above on the new thread and let’s take it from there.\n",
            "4.  yes  bet   extend     please         lets take   \n",
            "5. If I understand correctly, f[a,b] will always be either an interval of evens or an interval of odds? That is, it’s never possible to arrive at something like f[a,b]={4,0}?\n",
            "5.   understand correctly  will always  either             never possible    something  \n",
            "6. In (2), the  on the RHS should be  . In the statement of Theorem 9, you may wish to include the assumption  . In the statement of Lemma 13, you may wish to include the assumption that  . [Corrected, thanks – T.]\n",
            "6.        should      statement     may wish    assumption     statement     may wish    assumption       \n",
            "7. I was using the restriction  as well, I forgot to write this down. Could you explain what you mean by the eigenfunction equation for  made the original optimal weights a stationary point? I think my calculations should be giving an improvement without the  trick (from 1.84 to 1.87), which maybe means that I’ve made a mistake and this doesn’t really work. I see that this gives no benefit if  is constant with respect to  , but the optimal small gaps weights aren’t constant in  , so I would have guessed a win here.\n",
            "7.   using       forgot     could  explain   mean       made  original       think   should  giving  improvement without         maybe means   made      really work  see   gives        respect                 would  guessed  win \n",
            "8. Initially I’m wondering about the range of radii that allow a 7-colouring of a buckyball, i.e. a truncated icosahedron and its extensions. I haven’t put pen to paper, but I feel that surely they must plug the gap between the 6-colorable dodecahedron and the plane. Are the pentagons more problematic than I’m visualising?\n",
            "8.   wondering             ie               feel  surely  must                  \n",
            "9. In Section 2 of this post,  should be  (3 occurrences). Correspondingly, in the text, “at most  ” should be “at least  “. [Corrected, thanks – T.]\n",
            "9.        should          most   should         \n",
            "10. I would have a couple of questions regarding this very interesting conjecture. 1, Why is affinely independent needed? 2, In the conclusion, why are you asking for an arbitrary point in common instead of the origin? 3, Is the condition that the origin is in the convex hulls known to be needed for the conclusion? (I know that a topological version of Tverberg’s theorem has been disproved, but the current statement is about convex hulls.)\n",
            "10.  would     questions    interesting       needed    conclusion    asking   arbitrary    instead       condition         known   needed   conclusion  know              statement    \n",
            "\n",
            "Topic: 9\n",
            "1. Here’s some output of an adaptive mesh script based on the “second approach” derivative bounds wiki page, for t=0.4, x between 20 and 1000, y=0.4, digits=20. https://gist.githubusercontent.com/p15-git-acc/3ada0ff0b9ec77e23cb7cace0dcb8691/raw/807e1b0a16356a9bbd2a5af872f71bc064830c38/gistfile1.txt It took 3 or 4 hours to run. The “D” and “step” values should mean the same thing as in https://terrytao.wordpress.com/2018/03/02/polymath15-fifth-thread-finishing-off-the-test-problem/#comment-494166 . The output should have full accuracy, not just for the H values but also for the “D” and “step” values. If I understand correctly, other people have already covered this range of x, and furthermore the method I used is about to become obsolete due to the “third approach” for derivative bounds. For those reasons I haven’t yet spent any time turning the code into a single file that other people could build and run.\n",
            "1.         based    approach                            should mean          should   accuracy        also         understand correctly    already         method            approach      reasons   yet              could build  \n",
            "2. Yes. I almost have the solution (H satisfies a very simple second order equation!)\n",
            "2. yes  almost        simple   \n",
            "3. Tim, first of all I agree that the original argument works even for an incomplete partitioning. I don’t think that the argument is incomplete, I’m just slow following it. In this particular case, I think that I didn’t see the good bound on the number of iterations, I though that the fast growing number of partition classes might be a problem.\n",
            "3.      agree   original argument works    incomplete    think   argument  incomplete    following    particular case  think    see                   might   problem\n",
            "4. Yes, there are some functions in the repo Ittheta and Ktheta implemented (but not present in the main branch yet), where we try to do this. These still have to be sorted out completely. On the other hand, now that we have a approx functional eqn, and if we also get expansions of the first few error terms soon, then it could be much faster and still accurate enough compared to the expensive quadrature based approaches.\n",
            "4. yes                   yet   try           completely                also get           could      accurate enough compared     based approaches\n",
            "5. Yes, that works — as does Tim’s construction. And I think we can get an arbitrarily high ratio by letting  run over all  -element subsets, with  , and then subtracting  times  (this gives a ratio of about  for  ). I’m vaguely wondering about a type-6 approach where one starts with a solution in some larger set system and then tweaks the set system until one reaches a system of HAPs.\n",
            "5. yes  works     construction   think   get  arbitrarily                      gives          vaguely wondering    approach                        \n",
            "6. Here I counted edges of AC-type.\n",
            "6.      \n",
            "7. 90:  for small  (Re to 78 83 and 84)  : In 2 dimensions there are only 4 such set with 6 elements:  for i=1,2,3 and  . In 3 dimensions there is only one such set S with 18 elements. Proof: Let S be such a set. Now let  , for i=1,2,3. Now each  has to have 6 elements and be of the above from, and the intersection of the  ‘s has to be empty. Thus the  ‘s must be a permutation of the  ‘s. The only permutation, that does not give any combinatorial lines is  for i=1,2,3. Now asssume that  and let S be a set  in 4 dimensions and with no combinatorial lines. Again let  , for i=1,2,3. Now two of the  ‘s has to have 18 elements and thus be of the above form. By if two of them are identical, the third can at most have 27-18=9 elements, and S at most  . Contradiction.\n",
            "7.                                             proof let       let                              thus    must              give             let               let                 thus                 most       most   contradiction\n",
            "8. Here’s an idea which I think hasn’t been tried out yet (please correct me if I’m wrong). There’s a well-known sequence of intervals of consecutive integers of the form  , of length  . Let’s focus on the multiplicative case and fix a target discrepancy  (although one could also consider a direct approach to EDP). I’m wondering whether such intervals might force long strings of pluses (or minuses) whatever the multiplicative function one starts with. That is, whether one could obtain an upper bound on length as a function of  using precisely this family of consecutive integers. So, I’m wondering whether the prime factors that are used to construct the integers in some  might for “most of them” be in common with those used in many other  , ….,  ,…(perhaps choosing only some clever values for  ). If so, then maybe the constraints on each factor to be either + or – might imply that some  would necessarily have a long string of pluses. I’ll investigate this numerically later today (i.e. lists of factors and of constraints). For example, we know that for C=2 the longuest multiplicative sequences have length 246, but could it be that say  or  provide an upper bound anyway? (This is a re-post, sorry if this appears twice). (I really meant: intervals of consecutive composite integers of course.) I think this approach is yet another that fails the Sune test: we know that for  there will not be any long strings of pluses or minuses. I think you may have the problem that you have no control over the large prime factors in that interval. For example, in the interval  , as well as the prime factors less than 8, the primes 11, 13, 17, 47, 593, 823, 1613, 20161 and 13441 all enter as factors to odd powers. This gives a lot of freedom in the choice of signs. But perhaps I have missed something. I suppose that, strictly speaking, it is conceivable that one could prove the existence of a long string of pluses and minuses by assuming that the discrepancy of the sequence was bounded (and aiming for a contradiction). But such a proof would have to be an argument that worked for a bounded-discrepancy sequence and not for  . So, for instance, it would not be enough merely to assume complete multiplicativity. The detailed workings so far in low discrepancy cases seem to show that it is having to control the discrepancy in a number of such intervals simultaneously which creates the constraints. There are likely to be more constraints on low-value primes than on higher values, but quite often the patterns which are established deal with the low value issues, and it is higher value primes where the contradictions actually arise. It would take a very long interval for the information within the interval to create mutual constraints between a variety of primes. If the interval is not long enough then it seems to me that the information has to come from ‘outside’.\n",
            "8.   idea   think   tried  yet please correct    wrong theres                  lets focus    case  fix     although  could also consider   approach    wondering whether   might                 whether  could obtain           using precisely        wondering whether        construct      might  most                perhaps            maybe        either    might imply    would necessarily             ie        example  know            could    say    provide         sorry   appears   really meant         think  approach  yet        know     will           think  may   problem               example                                   gives           perhaps    something  suppose         could prove  existence          assuming             contradiction    proof would     argument                would   enough   assume           cases seem  show                      likely              often                     actually   would take       information                   enough   seems     information   come  \n",
            "9. NOT :  BUT  other typos are too many to be corrected, I hope that it is understandable with the (bad) english comments… (Note, without connection with the previous post, that I have a conjecture that is stronger than weaker FUNC (remplacing 1/2 by any c smaller then 1) here : https://mathoverflow.net/questions/290906/abundance-of-full-couples and that there is a few nice things to discuss futher about it…\n",
            "9.               hope       bad    without             stronger                     nice things  discuss   \n",
            "10. Eytan, I only used degree 1. It appears that when using larger degree, you can get close to the number predicted by your work.\n",
            "10.        appears   using     get        work\n"
          ]
        }
      ]
    }
  ]
}